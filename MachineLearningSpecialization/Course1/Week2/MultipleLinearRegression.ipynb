{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multiple Features"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are introducing new notation when we have multiple features $x_j$ for the jth feature, n is the total number of features"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now as before $x^i$ will be the ith training example, so the jth deature of this ith training example may be written as $x_j^i$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "also this training example can now be written in form of a vector as $\\vec{x}^i$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now our model from a $f_{w, b}$ will turn to\n",
    "$$f_{w, b}(x) = w_1x_1 + w_2x_2 + ... + w_nx_n + b$$\n",
    "\n",
    "In the vector notation this may be written as\n",
    "$$f_{\\vec{w}, b}(\\vec{x}) = \\vec{w}\\cdot\\vec{x} + b$$\n",
    "\n",
    "where,\n",
    "$$\\vec{w} = \\begin{bmatrix} w_1 \\ w_2 \\ ... \\ w_n \\end{bmatrix} \\space\\space\\space\\space and \\space\\space\\space\\space \\vec{x} = \\begin{bmatrix} x_1 \\ x_2\\ ... \\ x_n \\end{bmatrix} $$ \n",
    "are the row vectors"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = np.array([1.0, 2.5, -3.3])\n",
    "b = 4\n",
    "x = np.array([10, 20, 30])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Without vectorisation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$f_{\\vec{w}, b}(\\vec{x}) = \\bigg(\\sum_{j=1}^{n}w_jx_j\\bigg)+b$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = 0\n",
    "for j in range(0, len(x)):\n",
    "    f = f + w[j]*x[j]\n",
    "f = f + b"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = np.dot(w, x) + b"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Descent"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By now we know the algorithm for gradient descent is\n",
    "$$w_j = w_j - \\alpha \\frac{\\partial}{\\partial w_j} J$$\n",
    "where J is a function of w and b, in form of a cost function\n",
    "$$J(w, b) = \\frac{1}{2m}\\sum_{i=1}^n (\\hat{y}-y)^2$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*In Multi linear Regression*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = np.array([0.5, 1.3, 3.4, -6.6])\n",
    "d = np.array([0.3, 0.2, 0.1, 0.4])\n",
    "a = 0.01\n",
    "w = w - a*d"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NumPy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "np.zeros(4) :   a = [0. 0. 0. 0.], a shape = (4,), a data type = float64\n",
      "np.zeros(4,) :  a = [0. 0. 0. 0.], a shape = (4,), a data type = float64\n",
      "np.random.random_sample(4): a = [0.87071798 0.12786487 0.02801432 0.92533758], a shape = (4,), a data type = float64\n"
     ]
    }
   ],
   "source": [
    "a = np.zeros(4);                print(f\"np.zeros(4) :   a = {a}, a shape = {a.shape}, a data type = {a.dtype}\")\n",
    "a = np.zeros((4,));             print(f\"np.zeros(4,) :  a = {a}, a shape = {a.shape}, a data type = {a.dtype}\")\n",
    "a = np.random.random_sample(4); print(f\"np.random.random_sample(4): a = {a}, a shape = {a.shape}, a data type = {a.dtype}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "np.arange(4.):     a = [0. 1. 2. 3.], a shape = (4,), a data type = float64\n",
      "np.random.rand(4): a = [0.68640738 0.76972181 0.52036788 0.54925913], a shape = (4,), a data type = float64\n"
     ]
    }
   ],
   "source": [
    "a = np.arange(4.);              print(f\"np.arange(4.):     a = {a}, a shape = {a.shape}, a data type = {a.dtype}\")\n",
    "a = np.random.rand(4);          print(f\"np.random.rand(4): a = {a}, a shape = {a.shape}, a data type = {a.dtype}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 2 3 4 5 6 7 8 9]\n",
      "a[2].shape: () a[2]  = 2, Accessing an element returns a scalar\n",
      "a[-1] = 9\n",
      "The error message you'll see is:\n",
      "index 10 is out of bounds for axis 0 with size 10\n"
     ]
    }
   ],
   "source": [
    "#vector indexing operations on 1-D vectors\n",
    "a = np.arange(10)\n",
    "print(a)\n",
    "\n",
    "#access an element\n",
    "print(f\"a[2].shape: {a[2].shape} a[2]  = {a[2]}, Accessing an element returns a scalar\")\n",
    "\n",
    "# access the last element, negative indexes count from the end\n",
    "print(f\"a[-1] = {a[-1]}\")\n",
    "\n",
    "#indexs must be within the range of the vector or they will produce and error\n",
    "try:\n",
    "    c = a[10]\n",
    "except Exception as e:\n",
    "    print(\"The error message you'll see is:\")\n",
    "    print(e)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Single vector operations*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a             : [1 2 3 4]\n",
      "b = -a        : [-1 -2 -3 -4]\n",
      "b = np.sum(a) : 10\n",
      "b = np.mean(a): 2.5\n",
      "b = a**2      : [ 1  4  9 16]\n"
     ]
    }
   ],
   "source": [
    "a = np.array([1,2,3,4])\n",
    "print(f\"a             : {a}\")\n",
    "# negate elements of a\n",
    "b = -a \n",
    "print(f\"b = -a        : {b}\")\n",
    "\n",
    "# sum all elements of a, returns a scalar\n",
    "b = np.sum(a) \n",
    "print(f\"b = np.sum(a) : {b}\")\n",
    "\n",
    "b = np.mean(a)\n",
    "print(f\"b = np.mean(a): {b}\")\n",
    "\n",
    "b = a**2\n",
    "print(f\"b = a**2      : {b}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Dot Products*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_dot(a, b): \n",
    "    \"\"\"\n",
    "   Compute the dot product of two vectors\n",
    " \n",
    "    Args:\n",
    "      a (ndarray (n,)):  input vector \n",
    "      b (ndarray (n,)):  input vector with same dimension as a\n",
    "    \n",
    "    Returns:\n",
    "      x (scalar): \n",
    "    \"\"\"\n",
    "    x=0\n",
    "    for i in range(a.shape[0]):\n",
    "        x = x + a[i] * b[i]\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "my_dot(a, b) = 24\n"
     ]
    }
   ],
   "source": [
    "# test 1-D\n",
    "a = np.array([1, 2, 3, 4])\n",
    "b = np.array([-1, 4, 3, 2])\n",
    "print(f\"my_dot(a, b) = {my_dot(a, b)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NumPy 1-D np.dot(a, b) = 24, np.dot(a, b).shape = () \n"
     ]
    }
   ],
   "source": [
    "# test 1-D\n",
    "a = np.array([1, 2, 3, 4])\n",
    "b = np.array([-1, 4, 3, 2])\n",
    "c = np.dot(a, b)\n",
    "print(f\"NumPy 1-D np.dot(a, b) = {c}, np.dot(a, b).shape = {c.shape} \") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "np.dot(a, b) =  2501072.5817\n",
      "Vectorized version duration: 7.9827 ms \n",
      "my_dot(a, b) =  2501072.5817\n",
      "loop version duration: 3687.2375 ms \n"
     ]
    }
   ],
   "source": [
    "np.random.seed(1)\n",
    "a = np.random.rand(10000000)  # very large arrays\n",
    "b = np.random.rand(10000000)\n",
    "\n",
    "tic = time.time()  # capture start time\n",
    "c = np.dot(a, b)\n",
    "toc = time.time()  # capture end time\n",
    "\n",
    "print(f\"np.dot(a, b) =  {c:.4f}\")\n",
    "print(f\"Vectorized version duration: {1000*(toc-tic):.4f} ms \")\n",
    "\n",
    "tic = time.time()  # capture start time\n",
    "c = my_dot(a,b)\n",
    "toc = time.time()  # capture end time\n",
    "\n",
    "print(f\"my_dot(a, b) =  {c:.4f}\")\n",
    "print(f\"loop version duration: {1000*(toc-tic):.4f} ms \")\n",
    "\n",
    "del(a);del(b)  #remove these big arrays from memory"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Matrices in NumPy*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a shape = (1, 5), a = [[0. 0. 0. 0. 0.]]\n",
      "a shape = (2, 1), a = [[0.]\n",
      " [0.]]\n",
      "a shape = (2, 2), a = [[0.04997798 0.77390955]\n",
      " [0.93782363 0.5792328 ]]\n"
     ]
    }
   ],
   "source": [
    "a = np.zeros((1, 5))                                       \n",
    "print(f\"a shape = {a.shape}, a = {a}\")                     \n",
    "\n",
    "a = np.zeros((2, 1))                                                                   \n",
    "print(f\"a shape = {a.shape}, a = {a}\") \n",
    "\n",
    "a = np.random.random_sample((2, 2))  \n",
    "print(f\"a shape = {a.shape}, a = {a}\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a.shape: (3, 2), \n",
      "a= [[0 1]\n",
      " [2 3]\n",
      " [4 5]]\n",
      "\n",
      "a[2,0].shape:   (), a[2,0] = 4,     type(a[2,0]) = <class 'numpy.int32'> Accessing an element returns a scalar\n",
      "\n",
      "a[2].shape:   (2,), a[2]   = [4 5], type(a[2])   = <class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "#vector indexing operations on matrices\n",
    "a = np.arange(6).reshape(-1, 2)   #reshape is a convenient way to create matrices\n",
    "print(f\"a.shape: {a.shape}, \\na= {a}\")\n",
    "\n",
    "#access an element\n",
    "print(f\"\\na[2,0].shape:   {a[2, 0].shape}, a[2,0] = {a[2, 0]},     type(a[2,0]) = {type(a[2, 0])} Accessing an element returns a scalar\\n\")\n",
    "\n",
    "#access a row\n",
    "print(f\"a[2].shape:   {a[2].shape}, a[2]   = {a[2]}, type(a[2])   = {type(a[2])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a = \n",
      "[[ 0  1  2  3  4  5  6  7  8  9]\n",
      " [10 11 12 13 14 15 16 17 18 19]]\n",
      "a[0, 2:7:1] =  [2 3 4 5 6] ,  a[0, 2:7:1].shape = (5,) a 1-D array\n",
      "a[:, 2:7:1] = \n",
      " [[ 2  3  4  5  6]\n",
      " [12 13 14 15 16]] ,  a[:, 2:7:1].shape = (2, 5) a 2-D array\n",
      "a[:,:] = \n",
      " [[ 0  1  2  3  4  5  6  7  8  9]\n",
      " [10 11 12 13 14 15 16 17 18 19]] ,  a[:,:].shape = (2, 10)\n",
      "a[1,:] =  [10 11 12 13 14 15 16 17 18 19] ,  a[1,:].shape = (10,) a 1-D array\n",
      "a[1]   =  [10 11 12 13 14 15 16 17 18 19] ,  a[1].shape   = (10,) a 1-D array\n"
     ]
    }
   ],
   "source": [
    "#vector 2-D slicing operations\n",
    "a = np.arange(20).reshape(-1, 10)\n",
    "print(f\"a = \\n{a}\")\n",
    "\n",
    "#access 5 consecutive elements (start:stop:step)\n",
    "print(\"a[0, 2:7:1] = \", a[0, 2:7:1], \",  a[0, 2:7:1].shape =\", a[0, 2:7:1].shape, \"a 1-D array\")\n",
    "\n",
    "#access 5 consecutive elements (start:stop:step) in two rows\n",
    "print(\"a[:, 2:7:1] = \\n\", a[:, 2:7:1], \",  a[:, 2:7:1].shape =\", a[:, 2:7:1].shape, \"a 2-D array\")\n",
    "\n",
    "# access all elements\n",
    "print(\"a[:,:] = \\n\", a[:,:], \",  a[:,:].shape =\", a[:,:].shape)\n",
    "\n",
    "# access all elements in one row (very common usage)\n",
    "print(\"a[1,:] = \", a[1,:], \",  a[1,:].shape =\", a[1,:].shape, \"a 1-D array\")\n",
    "# same as\n",
    "print(\"a[1]   = \", a[1],   \",  a[1].shape   =\", a[1].shape, \"a 1-D array\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Descent for Multiple Linear Regression using Vectorisation "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Model*\n",
    "$$f_{w, b}(x) = w_1x_1 + w_2x_2 + ... + w_nx_n + b$$\n",
    "Or,\n",
    "$$f_{\\vec{w}, b}(\\vec{x}) = \\vec{w} \\cdot \\vec{x} + b $$\n",
    "*Cost Function*\n",
    "$$J(w_1, w_2, ... ,w_n, b)$$\n",
    "Or,\n",
    "$$J(\\vec{w}, b)$$\n",
    "*Gradient Descent*\n",
    "$$\\vec{w} = \\vec{w} - \\alpha \\frac{\\partial}{\\partial \\vec{w}}J(\\vec{w}, b)$$\n",
    "$$b = b - \\alpha \\frac{\\partial}{\\partial b}J(\\vec{w}, b)$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Upon solving we can arrive at,\n",
    "$$w_j = w_j - \\alpha \\frac{1}{m}\\sum_{j=1}^m \\bigg(f_{\\vec{w}, b}(\\vec{x}^{(i)}) - y^{(i)} \\bigg)x_j^{(i)}$$\n",
    "$$b = b - \\alpha \\frac{1}{m}\\sum_{j=1}^m \\bigg(f_{\\vec{w}, b}(\\vec{x}^{(i)}) - y^{(i)} \\bigg)$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*An alternative to gradient descent*\n",
    "\n",
    "Normal equation\n",
    "- Only for linear regression\n",
    "- Solve for w, b without iterations\n",
    "\n",
    "Disadvantages\n",
    "- Doesn't generalize to other learning algorithms.\n",
    "- Slow when number of features is large (> 10,000)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
