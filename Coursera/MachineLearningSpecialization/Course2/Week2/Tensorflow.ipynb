{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Digit Recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dense, LeakyReLU\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.activations import sigmoid, relu, linear, softmax\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as GridSpec\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(1234) # for consistent results\n",
    "model = Sequential(\n",
    "    [               \n",
    "        tf.keras.Input(shape = (400,)),\n",
    "        Dense(units = 25, activation = 'relu'),\n",
    "        Dense(units = 15, activation = 'relu'),\n",
    "        Dense(units = 10, activation = 'linear')\n",
    "    ], name = \"my_model\" \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.losses import BinaryCrossentropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array()\n",
    "Y = np.array()\n",
    "model.compile(loss = BinaryCrossentropy())\n",
    "model.fit(X, Y, epochs=100)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Details about training a model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. specify how to compute output given input x and parameters w, b (define model)\n",
    "2. specify the loss and the cost function\n",
    "3. Train the model to minimize the cost function by changing the parameters\n",
    "\n",
    "Done by:\n",
    "\n",
    "1. model = Sequential([])\n",
    "2. model.compile(loss = BinaryCrossentrpy()) (same one we used for logistic regression)\n",
    "3. model.fit(X, y, epoch = 100)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Alternatives to sigmoid function"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ReLU $\\longrightarrow$ Rectified Linear Unit\n",
    "$$g(z) = max(0, z)$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to choose activation function"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Output\n",
    "It is very stright forward to take a function based on the type of output that is required when it comes to what function to take, like for example:\n",
    "1) When predicting probability, choose sigmoid function\n",
    "2) When predicting stock price, choose linear function\n",
    "3) When  predicting house price, choose ReLU\n",
    "\n",
    "#### Hidden layers\n",
    "ReLU is the most common choice, for hidden layers"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why do we need activation functions?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because without it, the model is nothing but a linear regression model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiclass Classification"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The formula is:\n",
    "Let $a_1, a_2, a_3, a_4$ be the four categories, then:\n",
    "$$a_1 = \\frac{e^{z_1}}{e^{z_1} + e^{z_2} + e^{z_3} + e^{z_4}}$$\n",
    "and similarly other estimates"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In general:\n",
    "$$z_j = \\vec{w}_j \\cdot \\vec{x} + b_j$$\n",
    "and,\n",
    "$$a_j = \\frac{e^{z_j}}{\\sum_{k=1}^{N}e^{z_k}}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_softmax(z):  \n",
    "    m = z.shape[0]\n",
    "    a = np.zeros(m)\n",
    "    e_z = np.exp(z)\n",
    "    Sum = np.sum(e_z)\n",
    "    for i in range(m):\n",
    "        a[i] = e_z[i]/Sum\n",
    "    return a"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cost function for softmax regression"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We Remember the Binary Crossentropy loss function, now we generalise the loss function here as:\n",
    "$$loss = \n",
    "    \\begin{cases}\n",
    "        -log(a_1) & \\text{if  } y = 1 \\\\\n",
    "        -log(a_2) & \\text{if  } y = 2 \\\\\n",
    "        & \\vdots \\\\\n",
    "        -log(a_N) & \\text{if  } y = N \\\\\n",
    "    \\end{cases}$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential([\n",
    "    Dense(units=25, activation='relu'),\n",
    "    Dense(units=15, activation='relu'),\n",
    "    Dense(units=10, activation='softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.losses import SparseCategoricalCrossentropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss = SparseCategoricalCrossentropy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X, Y, epoch = 100)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(this code will work, but there is a better version of this code, which we will se later so hold your horses)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Improved Implementation of softmax"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the memory in the computer is limited, there maybe more or less round of error or the floating point error, depending on the way you calculate as..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 0.000200000000000000\n"
     ]
    }
   ],
   "source": [
    "x = 2/10000\n",
    "print(f\"{x: .18f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 0.000199999999999978\n"
     ]
    }
   ],
   "source": [
    "x = (1+ 1/10000) - (1- 1/10000)\n",
    "print(f\"{x: .18f}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And the way to caluclate softmax which reduces these errors is..."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Logistic Regression:__\n",
    "\n",
    "so instead of: <br>\n",
    "model.compile(loss = BinaryCrossEntropy())\n",
    "\n",
    "we write: <br>\n",
    "model.compile(loss = BinaryCrossEntropy(from_logits = True))\n",
    "\n",
    "All this does is instead of calculating the $a_1$ and $a_2$'s seperately and then pulgging it in the loss function causing there to be floating point errors, we, now  directly put the formula of $a_1$ that is $\\frac{1}{1+e^{-z}}$ into the logarithmic expression which allows tensorflow to do the necessary adjustments and calculate the value with less floating point error"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Softmax Algorithm:__\n",
    "\n",
    "so instead of: <br>\n",
    "Dense(units = 10, activation = 'softmax') <br>\n",
    "model.compile(loss = SparseCategoricalCrossEntropy())\n",
    "\n",
    "we write: <br>\n",
    "Dense(units = 10, activation = 'linear') <br>\n",
    "model.compile(loss = SparseCategoricalCrossEntropy(from_logits = True))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the we write: <br>\n",
    "model.fit(X, Y, epoch = 100)\n",
    "\n",
    "__to predict:__ <br>\n",
    "logits = model(X) <br>\n",
    "f_x = tf.nn.softmax(logits)\n",
    "\n",
    "and here the logit are the \"zs\" and not the probability, and hence we add another line of code...\n",
    "\n",
    "this is also to be done for the logistic regression"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multilable Classification Problem"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, either we can train 3 multilayer perceptrons to do one task each, or, we can train one neuron to detec all three, by using a sigmoid activation function as the output and a output to be an n dimensional vector!"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advanced Optimization Algorithms\n",
    "Better then Gradient Descent"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Gradient Descent:__\n",
    "$$w_j = w_j - \\alpha \\frac{\\partial}{\\partial w_j} J(\\vec{w}, b)$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Adam Algorithm (Adaptive movement estimation):__ <br>\n",
    "Sometimes we wish to have a bigger learning rate and sometimes a smaller learning rate depending on the case, and this is automatically done by the Adam algorithm\n",
    "$$w_j = w_j - \\alpha_j \\frac{\\partial}{\\partial w_j}J(\\vec{w}, b)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer = tf.keras.optimizers.Adam(learning_rate = 1e-3),\n",
    "              loss = SparseCategoricalCrossentropy(from_logits = True))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Additional Layer Type\n",
    "Other than Dense Layer type"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Convolutional layer\n",
    "\n",
    "Here each neuron of the neural network dosen't look at all the data points, but rather only looks at a set region of the input data.\n",
    "- Faster Computation\n",
    "- Need less training data, and reduce overfitting"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
