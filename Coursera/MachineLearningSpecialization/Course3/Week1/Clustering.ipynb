{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-Means Clustering Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) First K means will guess where the cluster center is, known as a \"cluster centroid\", then algorithm will look at all the data, and whch whether it is closest to which cluster centroid\n",
    "2) Then the algorithm will move the red and blue guess centroid to the average of the other data sets, which were closer to either the red or the blue dot\n",
    "\n",
    "This is done over and over again\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimization Objective"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Notation__ <br>\n",
    "$$c^{(i)} = \\text{index of cluster (1, 2, ..., K) to which example } x^{(i)} \\text{ is currently assigned}$$\n",
    "$$\\mu_k = \\text{cluster centroid k}$$\n",
    "$$\\mu_{c^{(i)}} = \\text{cluster centroid of cluster to which example } x^{(i)} \\text{ has been assigned}$$\n",
    "\n",
    "$$J(c^{(i)}, ...c^{(m)}, \\mu_1, ..., \\mu_k) = \\frac{1}{m}\\sum_{i=1}^{m}||x^{(i)}-\\mu_{c^{(i)}}||$$\n",
    "This is called the __Distortion Cost Function__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The initialization is quite important to find the global minima and not get stuck in the local minima"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we rum multiple, (50-200) random initializations, to find the global minima"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choosing the number of clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are many methods to choose the number of clusters\n",
    "1. Elbow method\n",
    "    - Draw the cost function vs K and choose the \"elbow\" of the graph\n",
    "2. Generally we use these clusters for a later problem, and that gives us the number of the clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UNQ_C1\n",
    "# GRADED FUNCTION: find_closest_centroids\n",
    "\n",
    "def find_closest_centroids(X, centroids):\n",
    "    \"\"\"\n",
    "    Computes the centroid memberships for every example\n",
    "    \n",
    "    Args:\n",
    "        X (ndarray): (m, n) Input values      \n",
    "        centroids (ndarray): (K, n) centroids\n",
    "    \n",
    "    Returns:\n",
    "        idx (array_like): (m,) closest centroids\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    # Set K\n",
    "    K = centroids.shape[0]\n",
    "\n",
    "    # You need to return the following variables correctly\n",
    "    idx = np.zeros(X.shape[0], dtype=int)\n",
    "\n",
    "    ### START CODE HERE ###\n",
    "    for i in range(len(idx)):\n",
    "        k_val = 0\n",
    "        min_error = np.sum((X[i]-centroids[0])**2)\n",
    "        for j in range(K):\n",
    "            error = np.sum((X[i]-centroids[j])**2)\n",
    "            if min_error > error:\n",
    "                min_error = error\n",
    "                k_val = j\n",
    "        idx[i] = k_val\n",
    "     ### END CODE HERE ###\n",
    "    \n",
    "    return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UNQ_C2\n",
    "# GRADED FUNCTION: compute_centroids\n",
    "\n",
    "def compute_centroids(X, idx, K):\n",
    "    \"\"\"\n",
    "    Returns the new centroids by computing the means of the \n",
    "    data points assigned to each centroid.\n",
    "    \n",
    "    Args:\n",
    "        X (ndarray):   (m, n) Data points\n",
    "        idx (ndarray): (m,) Array containing index of closest centroid for each \n",
    "                       example in X. Concretely, idx[i] contains the index of \n",
    "                       the centroid closest to example i\n",
    "        K (int):       number of centroids\n",
    "    \n",
    "    Returns:\n",
    "        centroids (ndarray): (K, n) New centroids computed\n",
    "    \"\"\"\n",
    "    \n",
    "    # Useful variables\n",
    "    m, n = X.shape\n",
    "    \n",
    "    # You need to return the following variables correctly\n",
    "    centroids = np.zeros((K, n))\n",
    "    \n",
    "    ### START CODE HERE ###\n",
    "    for i in range(K):\n",
    "        X_closest = X[idx==i]\n",
    "        m = X_closest.shape[0]\n",
    "        for j in range(m):\n",
    "            centroids[i] += X_closest[j]\n",
    "        centroids[i] /= m\n",
    "    ### END CODE HERE ## \n",
    "    \n",
    "    return centroids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You do not need to implement anything for this part\n",
    "\n",
    "def run_kMeans(X, initial_centroids, max_iters=10, plot_progress=False):\n",
    "    \"\"\"\n",
    "    Runs the K-Means algorithm on data matrix X, where each row of X\n",
    "    is a single example\n",
    "    \"\"\"\n",
    "    \n",
    "    # Initialize values\n",
    "    m, n = X.shape\n",
    "    K = initial_centroids.shape[0]\n",
    "    centroids = initial_centroids\n",
    "    previous_centroids = centroids    \n",
    "    idx = np.zeros(m)\n",
    "    plt.figure(figsize=(8, 6))\n",
    "\n",
    "    # Run K-Means\n",
    "    for i in range(max_iters):\n",
    "        \n",
    "        #Output progress\n",
    "        print(\"K-Means iteration %d/%d\" % (i, max_iters-1))\n",
    "        \n",
    "        # For each example in X, assign it to the closest centroid\n",
    "        idx = find_closest_centroids(X, centroids)\n",
    "        \n",
    "        # Optionally plot progress\n",
    "        if plot_progress:\n",
    "            plot_progress_kMeans(X, centroids, previous_centroids, idx, K, i)\n",
    "            previous_centroids = centroids\n",
    "            \n",
    "        # Given the memberships, compute new centroids\n",
    "        centroids = compute_centroids(X, idx, K)\n",
    "    plt.show() \n",
    "    return centroids, idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You do not need to modify this part\n",
    "\n",
    "def kMeans_init_centroids(X, K):\n",
    "    \"\"\"\n",
    "    This function initializes K centroids that are to be \n",
    "    used in K-Means on the dataset X\n",
    "    \n",
    "    Args:\n",
    "        X (ndarray): Data points \n",
    "        K (int):     number of centroids/clusters\n",
    "    \n",
    "    Returns:\n",
    "        centroids (ndarray): Initialized centroids\n",
    "    \"\"\"\n",
    "    \n",
    "    # Randomly reorder the indices of examples\n",
    "    randidx = np.random.permutation(X.shape[0])\n",
    "    \n",
    "    # Take the first K examples as centroids\n",
    "    centroids = X[randidx[:K]]\n",
    "    \n",
    "    return centroids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
