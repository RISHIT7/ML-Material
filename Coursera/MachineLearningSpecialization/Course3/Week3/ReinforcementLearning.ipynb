{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reinforcemnt Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find the function of f that maps state s to an action a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) Supervised learning dosent work, go figure \n",
    "2) so we use Reinforcement learning\n",
    "\n",
    "And it is similar on how to make a puppy behave well, by giving treat when the puppy does good, and calling him a \"bad dog\" if not"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a helicopter:\n",
    "1) positive reward: helicopter flying well +1/s\n",
    "2) negative reward: helicopter flying poorly or crashing then -1000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mars rover example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's say the rover can be in the six positions called the state in reinforcement learning, these are:\n",
    "$$state \\begin{bmatrix}1 &|& 2 &|& 3 &|& 4 &|& 5 &|& 6 \\end{bmatrix}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's say the rover starts at state 4 and state 1 has an interesting land that the scientist would want to study, and so does state 6 but not so much as state 1, and so we would reflect state 1 to be more valuable would be 100, state 6 would be 40, and rest of the states will have 0 state, and the rest would be left to the robot\n",
    "\n",
    "The robot might go left, where it will get first, 0 then 0, 0, 0, and then 100 and then the day ends, or the _terminal state_, if not the robot could go to the right and get the following awards 0, 0, 40, but other than that, robot could go to first the right, and then go to the left then it'd happen like 0,0,0,0,0,100\n",
    "\n",
    "so for every state there is (state, action, reward(state), next_state), aka (s, a, R(s), s')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The return in Reinforcement learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The return is defined as the list of reward wieghted by a \"discount factor\" For example: <br>\n",
    "For a list of 0 0 0 100, the return would be\n",
    "\n",
    "$return = 0+(0.9)*0+(0.9)^2*0+(0.9)^3*100$\n",
    "__In general:__\n",
    "$$\\text{Return} = R_1 + \\gamma R_2 + \\gamma^2 R_3 + ... (\\text{until terminal state})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policy in reinforcement learning\n",
    "1) Either always go right\n",
    "2) Always go left\n",
    "3) Go towards the higher award\n",
    "4) go towards the lower reward\n",
    "5) go towards the higher reward except when you are one step away from the smaller reward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Policy $\\pi$ takes a state _s_ and maps to the action _a_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is known as Markov Decision Process (MDP)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
