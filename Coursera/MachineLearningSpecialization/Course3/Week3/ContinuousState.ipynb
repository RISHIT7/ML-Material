{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Discrete vs Continuous State"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's say we are coding a helicopter, in this case the state is a vector of 6 values, that is: _x, y, z, row, pitch, yaw_\n",
    "$$s = \\begin{bmatrix} x \\\\ y \\\\ z \\\\ \\phi \\\\ \\theta \\\\ \\omega \\\\ \\dot{x} \\\\ \\dot{y} \\\\ \\dot{z} \\\\ \\dot{\\phi} \\\\ \\dot{\\theta} \\\\ \\dot{\\omega} \\end{bmatrix}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lunar Lander"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__The state of our system is__\n",
    "$$state = \\begin{bmatrix} x \\\\ y \\\\ \\theta \\\\ \\dot{x} \\\\ \\dot{y} \\\\ \\dot{\\theta} \\\\ l \\\\ r\\end{bmatrix}$$\n",
    "__The actions will be:__\n",
    "1) do nothing\n",
    "2) left thruster\n",
    "3) main thruster\n",
    "4) right thruster <br>\n",
    "\n",
    "__Reward function:__\n",
    "- Getting to landing pad: 100 to 140\n",
    "- Additional reward for moving towards/away from pad\n",
    "- Crash: -100\n",
    "- Soft landing: 100\n",
    "- Leg grounded: 10\n",
    "- Fire main engine: -0.3\n",
    "- Fire side thruster: -0.03"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning the state-action value function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a state s, use neural network to compute the following:\n",
    "1) Q(s, nothing)\n",
    "2) Q(s, left)\n",
    "3) Q(s, main)\n",
    "4) Q(s, right)\n",
    "\n",
    "Pick the action that maximizes Q(s, a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To train the neural network we use the Bellman Equation\n",
    "$$Q(s, a) = R(s) + \\gamma \\max_{a'}Q(s', a')$$\n",
    "\n",
    "First to train random things, and observe what we did, and what actions we took, and what reward we got... and we will take a mark of the tuples that maybe <br> $(s^{(1)}, a^{(1)}, R(s^{(1)}), s'^{(1)})$, $(s^{(2)}, a^{(2)}, R(s^{(2)}), s'^{(2)})$, $(s^{(3)}, a^{(3)}, R(s^{(3)}), s'^{(3)})$ and $(s^{(4)}, a^{(4)}, R(s^{(4)}), s'^{(4)})$<br> and this will be enough for us to know the neural network parameters\n",
    "\n",
    "As now the input would just be $$x = (s^{(1)}, a^{(1)})$$ and $$y = R(s) + \\gamma \\max_{a'}Q(s', a')$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize neural network randomly as guess of Q(s, a), where we train 4  neural network, each for one action, with one output layer\n",
    "\n",
    "Repeat { <br>\n",
    "    Take actions in the lunar lander. Get(s, a, R(s), s'), <br>\n",
    "    Store 10,000 most recent (s, a, R(s), s') <- _Replay Buffer_ <br>\n",
    "}\n",
    "\n",
    "- Train neural network:\n",
    "    - Create training set of 10,000 examples using\n",
    "        - $x=(s, a)$ and $y=R(s)+ \\gamma \\max_{a'}Q(s', a')$\n",
    "    - Train $Q_{new}$ such that $Q_{new}(s, a) \\approx y$\n",
    "- Set $Q = Q_{new}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Algorithm refinement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well go figure, turns out, training 4 neural networks is not the best algorithm and its better to have an output layer with 4 neurons lol"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### $\\epsilon$-greedy policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In some state s\n",
    "- Option 1:\n",
    "    - Pick the action a that maximizes $Q(s, a)$\n",
    "- Option 2:\n",
    "    - With probability 0.95, pick the action a that maximizes $Q(s, a)$\n",
    "    - with probability 0.05, pick an action a randomly\n",
    "\n",
    "Option 1 may work okay but isn't the best option, but Option 2 works!, this is called the __Exploration step__ or,__. $\\epsilon$ -greedy policy__ Option 1 is called the __Exploitation step__ or, __Greedy method__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's better to start with a high $\\epsilon$ and then gradually decrease, so initially we pick random actions, and then we take greedy actions later so we learn what every action does"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mini-batch and soft updates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mini-batches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's consider the house pricing problem from firest course, what if m is a hundred million, when m is large, the algorithm becomes very slow,\n",
    "\n",
    "So in mini-batch algorithm, we take a subset of the large set, let's say m' = 1000, and then each iteration through gradient descent we only take 1000, and the subset keeps __changing__ every iteration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So in case of Reinforcement learning, even though we have stored 10,000 most recent tuples, we may not use all of them to train the model, and may only use a 1,000 of them to train the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Soft updates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rather than setting the $Q = Q_{new}$\n",
    "\n",
    "But with the soft update we set, \n",
    "$$w = 0.01w_{new} + 0.99w$$\n",
    "$$b = 0.01b_{new} + 0.99b$$"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
