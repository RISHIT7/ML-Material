{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Auto Diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = tf.Variable(3.0)\n",
    "x = 1.0\n",
    "y = 1.0\n",
    "alpha = 0.01\n",
    "\n",
    "iterations = 30\n",
    "for iter in range(iterations):\n",
    "    # tensorflow's gradient tape to record the steps\n",
    "    with tf.GradientTape() as tape:\n",
    "        fwb = w*x\n",
    "        costJ = (fwb - y)**2\n",
    "\n",
    "    [dJdw] = tape.gradient(costJ, [w])\n",
    "\n",
    "    w.assign_add(-alpha*dJdw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimizer = tf.keras.optimizers.Adam(learning_rate=0.1)\n",
    "\n",
    "# iterations = 200\n",
    "# for iter in range(iterations):\n",
    "#     with tf.GradientTape() as tape:\n",
    "\n",
    "#         cost_val = cofiCostFuncV(X, W, b, Ynorm, R, num_users, num_movies, lambda)\n",
    "    \n",
    "#     grads  = tape.gradient(cost_value, [x, w, b])\n",
    "\n",
    "#     optimizer.apply_gradients(zip(grads, [x, w, b]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding related items"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The features $x^{(i)}$ of item $i$ are quite hard to interpret, but collectively they do convey something about that movie, i, and to find realted items, we find a movie with similar features calcualted as,\n",
    "$$||x^{(k)}-x^{(i)}||^2$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Limitations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cold start problem\n",
    "- rank new items that few users have rated?\n",
    "- show something reasonable to new users who have rated few items?\n",
    "\n",
    "### Use side infromation about items or users:\n",
    "- Item: Genre, movie stars, studio,...\n",
    "- User: Demographics (age, gender, location), expressed preferences, ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: cofi_cost_func\n",
    "# UNQ_C1\n",
    "\n",
    "def cofi_cost_func(X, W, b, Y, R, lambda_):\n",
    "    \"\"\"\n",
    "    Returns the cost for the content-based filtering\n",
    "    Args:\n",
    "      X (ndarray (num_movies,num_features)): matrix of item features\n",
    "      W (ndarray (num_users,num_features)) : matrix of user parameters\n",
    "      b (ndarray (1, num_users)            : vector of user parameters\n",
    "      Y (ndarray (num_movies,num_users)    : matrix of user ratings of movies\n",
    "      R (ndarray (num_movies,num_users)    : matrix, where R(i, j) = 1 if the i-th movies was rated by the j-th user\n",
    "      lambda_ (float): regularization parameter\n",
    "    Returns:\n",
    "      J (float) : Cost\n",
    "    \"\"\"\n",
    "    nm, nu = Y.shape\n",
    "    J = 0\n",
    "    ### START CODE HERE ###  \n",
    "    for i in range(nm):\n",
    "        for j in range(nu):\n",
    "            prediction = np.dot(X[i], W[j]) + b[:, j]\n",
    "            error = ((prediction-Y[i][j])**2)*R[i][j]\n",
    "            J += (1/2)*error\n",
    "    for i in range(nm):\n",
    "        J += lambda_*(np.sum(X[i]**2))/2\n",
    "    for j in range(nu):\n",
    "        J += lambda_*(np.sum(W[j]**2))/2\n",
    "    J = J[0]\n",
    "    ### END CODE HERE ### \n",
    "\n",
    "    return J"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cofi_cost_func_v(X, W, b, Y, R, lambda_):\n",
    "    \"\"\"\n",
    "    Returns the cost for the content-based filtering\n",
    "    Vectorized for speed. Uses tensorflow operations to be compatible with custom training loop.\n",
    "    Args:\n",
    "      X (ndarray (num_movies,num_features)): matrix of item features\n",
    "      W (ndarray (num_users,num_features)) : matrix of user parameters\n",
    "      b (ndarray (1, num_users)            : vector of user parameters\n",
    "      Y (ndarray (num_movies,num_users)    : matrix of user ratings of movies\n",
    "      R (ndarray (num_movies,num_users)    : matrix, where R(i, j) = 1 if the i-th movies was rated by the j-th user\n",
    "      lambda_ (float): regularization parameter\n",
    "    Returns:\n",
    "      J (float) : Cost\n",
    "    \"\"\"\n",
    "    j = (tf.linalg.matmul(X, tf.transpose(W)) + b - Y)*R\n",
    "    J = 0.5 * tf.reduce_sum(j**2) + (lambda_/2) * (tf.reduce_sum(X**2) + tf.reduce_sum(W**2))\n",
    "    return J"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Content-Based Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Load Data, set configuration variables\n",
    "# item_train, user_train, y_train, item_features, user_features, item_vecs, movie_dict, user_to_genre = load_data()\n",
    "\n",
    "# num_user_features = user_train.shape[1] - 3  # remove userid, rating count and ave rating during training\n",
    "# num_item_features = item_train.shape[1] - 1  # remove movie id at train time\n",
    "# uvs = 3  # user genre vector start\n",
    "# ivs = 3  # item genre vector start\n",
    "# u_s = 3  # start of columns to use in training, user\n",
    "# i_s = 1  # start of columns to use in training, items\n",
    "# print(f\"Number of training vectors: {len(item_train)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # scale training data\n",
    "# item_train_unscaled = item_train\n",
    "# user_train_unscaled = user_train\n",
    "# y_train_unscaled    = y_train\n",
    "\n",
    "# scalerItem = StandardScaler()\n",
    "# scalerItem.fit(item_train)\n",
    "# item_train = scalerItem.transform(item_train)\n",
    "\n",
    "# scalerUser = StandardScaler()\n",
    "# scalerUser.fit(user_train)\n",
    "# user_train = scalerUser.transform(user_train)\n",
    "\n",
    "# scalerTarget = MinMaxScaler((-1, 1))\n",
    "# scalerTarget.fit(y_train.reshape(-1, 1))\n",
    "# y_train = scalerTarget.transform(y_train.reshape(-1, 1))\n",
    "# #ynorm_test = scalerTarget.transform(y_test.reshape(-1, 1))\n",
    "\n",
    "# print(np.allclose(item_train_unscaled, scalerItem.inverse_transform(item_train)))\n",
    "# print(np.allclose(user_train_unscaled, scalerUser.inverse_transform(user_train)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # GRADED_CELL\n",
    "# # UNQ_C1\n",
    "\n",
    "# num_outputs = 32\n",
    "# tf.random.set_seed(1)\n",
    "# user_NN = tf.keras.models.Sequential([\n",
    "#     ### START CODE HERE ###     \n",
    "#     tf.keras.layers.Dense(units= 256, activation= 'relu'),\n",
    "#     tf.keras.layers.Dense(units = 128, activation = 'relu'),\n",
    "#     tf.keras.layers.Dense(units = num_outputs, activation = 'linear')\n",
    "#     ### END CODE HERE ###  \n",
    "# ])\n",
    "\n",
    "# item_NN = tf.keras.models.Sequential([\n",
    "#     ### START CODE HERE ###     \n",
    "#     tf.keras.layers.Dense(units= 256, activation = 'relu'),\n",
    "#     tf.keras.layers.Dense(units = 128, activation = 'relu'),\n",
    "#     tf.keras.layers.Dense(units = num_outputs, activation = 'linear')\n",
    "#     ### END CODE HERE ###  \n",
    "# ])\n",
    "\n",
    "# # create the user input and point to the base network\n",
    "# input_user = tf.keras.layers.Input(shape=(num_user_features))\n",
    "# vu = user_NN(input_user)\n",
    "# vu = tf.linalg.l2_normalize(vu, axis=1)\n",
    "\n",
    "# # create the item input and point to the base network\n",
    "# input_item = tf.keras.layers.Input(shape=(num_item_features))\n",
    "# vm = item_NN(input_item)\n",
    "# vm = tf.linalg.l2_normalize(vm, axis=1)\n",
    "\n",
    "# # compute the dot product of the two vectors vu and vm\n",
    "# output = tf.keras.layers.Dot(axes=1)([vu, vm])\n",
    "\n",
    "# # specify the inputs and output of the model\n",
    "# model = tf.keras.Model([input_user, input_item], output)\n",
    "\n",
    "# model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf.random.set_seed(1)\n",
    "# cost_fn = tf.keras.losses.MeanSquaredError()\n",
    "# opt = keras.optimizers.Adam(learning_rate=0.01)\n",
    "# model.compile(optimizer=opt,\n",
    "#               loss=cost_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf.random.set_seed(1)\n",
    "# model.fit([user_train[:, u_s:], item_train[:, i_s:]], y_train, epochs=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.evaluate([user_test[:, u_s:], item_test[:, i_s:]], y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new_user_id = 5000\n",
    "# new_rating_ave = 0.0\n",
    "# new_action = 0.0\n",
    "# new_adventure = 5.0\n",
    "# new_animation = 0.0\n",
    "# new_childrens = 0.0\n",
    "# new_comedy = 0.0\n",
    "# new_crime = 0.0\n",
    "# new_documentary = 0.0\n",
    "# new_drama = 0.0\n",
    "# new_fantasy = 5.0\n",
    "# new_horror = 0.0\n",
    "# new_mystery = 0.0\n",
    "# new_romance = 0.0\n",
    "# new_scifi = 0.0\n",
    "# new_thriller = 0.0\n",
    "# new_rating_count = 3\n",
    "\n",
    "# user_vec = np.array([[new_user_id, new_rating_count, new_rating_ave,\n",
    "#                       new_action, new_adventure, new_animation, new_childrens,\n",
    "#                       new_comedy, new_crime, new_documentary,\n",
    "#                       new_drama, new_fantasy, new_horror, new_mystery,\n",
    "#                       new_romance, new_scifi, new_thriller]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # generate and replicate the user vector to match the number movies in the data set.\n",
    "# user_vecs = gen_user_vecs(user_vec,len(item_vecs))\n",
    "\n",
    "# # scale our user and item vectors\n",
    "# suser_vecs = scalerUser.transform(user_vecs)\n",
    "# sitem_vecs = scalerItem.transform(item_vecs)\n",
    "\n",
    "# # make a prediction\n",
    "# y_p = model.predict([suser_vecs[:, u_s:], sitem_vecs[:, i_s:]])\n",
    "\n",
    "# # unscale y prediction \n",
    "# y_pu = scalerTarget.inverse_transform(y_p)\n",
    "\n",
    "# # sort the results, highest prediction first\n",
    "# sorted_index = np.argsort(-y_pu,axis=0).reshape(-1).tolist()  #negate to get largest rating first\n",
    "# sorted_ypu   = y_pu[sorted_index]\n",
    "# sorted_items = item_vecs[sorted_index]  #using unscaled vectors for display\n",
    "\n",
    "# print_pred_movies(sorted_ypu, sorted_items, movie_dict, maxcount = 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- uid = 2 \n",
    "# form a set of user vectors. This is the same vector, transformed and repeated.\n",
    "user_vecs, y_vecs = get_user_vecs(uid, user_train_unscaled, item_vecs, user_to_genre)\n",
    "\n",
    "# scale our user and item vectors\n",
    "suser_vecs = scalerUser.transform(user_vecs)\n",
    "sitem_vecs = scalerItem.transform(item_vecs)\n",
    "\n",
    "# make a prediction\n",
    "y_p = model.predict([suser_vecs[:, u_s:], sitem_vecs[:, i_s:]])\n",
    "\n",
    "# unscale y prediction \n",
    "y_pu = scalerTarget.inverse_transform(y_p)\n",
    "\n",
    "# sort the results, highest prediction first\n",
    "sorted_index = np.argsort(-y_pu,axis=0).reshape(-1).tolist()  #negate to get largest rating first\n",
    "sorted_ypu   = y_pu[sorted_index]\n",
    "sorted_items = item_vecs[sorted_index]  #using unscaled vectors for display\n",
    "sorted_user  = user_vecs[sorted_index]\n",
    "sorted_y     = y_vecs[sorted_index]\n",
    "\n",
    "#print sorted predictions for movies rated by the user\n",
    "print_existing_user(sorted_ypu, sorted_y.reshape(-1,1), sorted_user, sorted_items, ivs, uvs, movie_dict, maxcount = 50) -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # GRADED_FUNCTION: sq_dist\n",
    "# # UNQ_C2\n",
    "# def sq_dist(a,b):\n",
    "#     \"\"\"\n",
    "#     Returns the squared distance between two vectors\n",
    "#     Args:\n",
    "#       a (ndarray (n,)): vector with n features\n",
    "#       b (ndarray (n,)): vector with n features\n",
    "#     Returns:\n",
    "#       d (float) : distance\n",
    "#     \"\"\"\n",
    "#     ### START CODE HERE ###     \n",
    "#     d = np.sum((a-b)**2)\n",
    "#     ### END CODE HERE ###     \n",
    "#     return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a1 = np.array([1.0, 2.0, 3.0]); b1 = np.array([1.0, 2.0, 3.0])\n",
    "# a2 = np.array([1.1, 2.1, 3.1]); b2 = np.array([1.0, 2.0, 3.0])\n",
    "# a3 = np.array([0, 1, 0]);       b3 = np.array([1, 0, 0])\n",
    "# print(f\"squared distance between a1 and b1: {sq_dist(a1, b1):0.3f}\")\n",
    "# print(f\"squared distance between a2 and b2: {sq_dist(a2, b2):0.3f}\")\n",
    "# print(f\"squared distance between a3 and b3: {sq_dist(a3, b3):0.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input_item_m = tf.keras.layers.Input(shape=(num_item_features))    # input layer\n",
    "# vm_m = item_NN(input_item_m)                                       # use the trained item_NN\n",
    "# vm_m = tf.linalg.l2_normalize(vm_m, axis=1)                        # incorporate normalization as was done in the original model\n",
    "# model_m = tf.keras.Model(input_item_m, vm_m)                                \n",
    "# model_m.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scaled_item_vecs = scalerItem.transform(item_vecs)\n",
    "# vms = model_m.predict(scaled_item_vecs[:,i_s:])\n",
    "# print(f\"size of all predicted movie feature vectors: {vms.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count = 50  # number of movies to display\n",
    "# dim = len(vms)\n",
    "# dist = np.zeros((dim,dim))\n",
    "\n",
    "# for i in range(dim):\n",
    "#     for j in range(dim):\n",
    "#         dist[i,j] = sq_dist(vms[i, :], vms[j, :])\n",
    "        \n",
    "# m_dist = ma.masked_array(dist, mask=np.identity(dist.shape[0]))  # mask the diagonal\n",
    "\n",
    "# disp = [[\"movie1\", \"genres\", \"movie2\", \"genres\"]]\n",
    "# for i in range(count):\n",
    "#     min_idx = np.argmin(m_dist[i])\n",
    "#     movie1_id = int(item_vecs[i,0])\n",
    "#     movie2_id = int(item_vecs[min_idx,0])\n",
    "#     disp.append( [movie_dict[movie1_id]['title'], movie_dict[movie1_id]['genres'],\n",
    "#                   movie_dict[movie2_id]['title'], movie_dict[movie1_id]['genres']]\n",
    "#                )\n",
    "# table = tabulate.tabulate(disp, tablefmt='html', headers=\"firstrow\")\n",
    "# table"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
