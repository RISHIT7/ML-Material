{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Underfitting and Overfitting"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Underfitting\n",
    "- The Model does not fit very  well.\n",
    "- AKA __has high bias__.\n",
    "\n",
    "Overfitting\n",
    "- Fits very well, but Overfits the data.\n",
    "- Does not generalize well, __Has high variance__.\n",
    "\n",
    "We want our algorithm to generalize well"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Addressing Overfiting"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Ways to solve the error, In priority order:__\n",
    "1) Get a Larger set of **training** example\n",
    "2) **Reducing the number of features**, all features +insufficient data gives overfiting, hence *feature selection* in necessary. In course 2 we weill learn some alogrithms to automatically choose the best features.\n",
    "3) **Regularisation**, causing the size of $w_j$ parameters to become smaller, and thus causing corresponding features to cause less of effect"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regularisation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In regularisation we add 1000 times the feature or the feautre squared to the cost function as in to penalise the cost function an thus automatically the algorithm will itself make the model have lower values of the features\n",
    "\n",
    "Now the problem arises when there are a large number of features like let's say a hundred features, then by practice is has been shows that penalising every feature, usually gives a much better fitting curve\n",
    "\n",
    "This is done by:\n",
    "For Linear Regression\n",
    "$$J(\\vec{w}, b) = \\frac{1}{2m}\\sum_{i=1}^{m}(f_{\\vec{w}, b}(\\vec{x}^{(i)}) - y^{(i)})^2 + \\frac{\\lambda}{2m}\\sum_{j=1}^{n}w_j^2$$\n",
    "\n",
    "For Logistic Regression:\n",
    "$$J(\\vec{w}, b) = -\\frac{1}{m}\\sum_{i=1}^{m}\\bigg[y^{(i)}log\\big(f_{\\vec{w}, b}(\\vec{x}^{(i)})\\big) + (1-y^{(i)})log\\big(1-f_{\\vec{w}, b}(\\vec{x}^{(i)})\\big)\\bigg] + \\frac{\\lambda}{2m}\\sum_{j=1}^{n}w_j^2$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regularisation in Linear Regression"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, \n",
    "$$\\frac{\\partial}{\\partial w_j}J(\\vec{w}, b) = \\frac{1}{m}\\sum_{i=1}^{m}(f_{\\vec{w}, b}(\\vec{x}^{(i)})-y^{(i)})x_j^{(i)} + \\frac{\\lambda}{m}w_j$$\n",
    "$$\\frac{\\partial}{\\partial b}J(\\vec{w}, b) = \\frac{1}{m}\\sum_{i=1}^{m}(f_{\\vec{w}, b}(\\vec{x}^{(i)})-y^{(i)})$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regularisation of Logistic Regression"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, \n",
    "$$\\frac{\\partial}{\\partial w_j}J(\\vec{w}, b) = \\frac{1}{m}\\sum_{i=1}^{m}(f_{\\vec{w}, b}(\\vec{x}^{(i)})-y^{(i)})x_j^{(i)} + \\frac{\\lambda}{m}w_j$$\n",
    "$$\\frac{\\partial}{\\partial b}J(\\vec{w}, b) = \\frac{1}{m}\\sum_{i=1}^{m}(f_{\\vec{w}, b}(\\vec{x}^{(i)})-y^{(i)})$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(X, w, b): \n",
    "    \"\"\"\n",
    "    Predict whether the label is 0 or 1 using learned logistic\n",
    "    regression parameters w\n",
    "    \n",
    "    Args:\n",
    "      X : (ndarray Shape (m,n)) data, m examples by n features\n",
    "      w : (ndarray Shape (n,))  values of parameters of the model      \n",
    "      b : (scalar)              value of bias parameter of the model\n",
    "\n",
    "    Returns:\n",
    "      p : (ndarray (m,)) The predictions for X using a threshold at 0.5\n",
    "    \"\"\"\n",
    "    # number of training examples\n",
    "    m, n = X.shape   \n",
    "    p = np.zeros(m)\n",
    "   \n",
    "    ### START CODE HERE ### \n",
    "    # Loop over each example\n",
    "    for i in range(m):   \n",
    "        z_wb = 0\n",
    "        # Loop over each feature\n",
    "        for j in range(n): \n",
    "            # Add the corresponding term to z_wb\n",
    "            z_wb += X[i, j]*w[j]\n",
    "        \n",
    "        # Add bias term \n",
    "        z_wb += b\n",
    "        \n",
    "        # Calculate the prediction for this example\n",
    "        f_wb = 1/(1+np.exp(-z_wb))\n",
    "\n",
    "        # Apply the threshold\n",
    "        if f_wb >= 0.5:\n",
    "            p[i] = 1\n",
    "        else:\n",
    "            p[i] = 0\n",
    "        \n",
    "    ### END CODE HERE ###Â \n",
    "    return p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
